{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('/Users/hamid/Group-06-Sentiment-Analysis-')\n",
    "sys.path.append('./scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API Key is: AIzaSyAfhKdB3DRT9XhRCukpl64bDDHNQVJU0E8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scripts.fetch_comments import fetch_comments, save_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 comments to ./Data/comments_new.csv\n",
      "Total comments fetched: 2000\n"
     ]
    }
   ],
   "source": [
    "video_id = \"JfVOs4VSpmA\"\n",
    "comments = fetch_comments(video_id,2000)\n",
    "save_to_csv(comments, \"./Data/comments_new.csv\")\n",
    "print(f\"Total comments fetched: {len(comments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaning_and_labeling import load_comments, filter_non_english_comments, filter_emoji_only_comments, save_cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 comments from ./Data/comments_new.csv\n",
      "Filtering non-English comments...\n",
      "Kept 1475 English comments.\n",
      "Removing emoji-only comments...\n",
      "Kept 1475 comments with text.\n",
      "Saved cleaned comments to ./Data/cleaned_comments.csv\n"
     ]
    }
   ],
   "source": [
    "input_filepath = \"./Data/comments_new.csv\"\n",
    "output_filepath = \"./Data/cleaned_comments.csv\"\n",
    "\n",
    "df = load_comments(input_filepath)\n",
    "df = filter_non_english_comments(df)\n",
    "df = filter_emoji_only_comments(df)\n",
    "save_cleaned_comments(df, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic labeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Sentiment\n",
      "0                                 It was 3 years ago   neutral\n",
      "1  Overall it&#39;s a perfect masterpiece<br>I wa...  positive\n",
      "2  I managed to get a tenth of the way through th...  negative\n",
      "3  I still remember when this trailer was release...   neutral\n",
      "4                      Whoâ€™s here for brand new day?  positive\n",
      "Labeled comments saved to ./Data/labeled_comments.csv\n"
     ]
    }
   ],
   "source": [
    "from basic_labeling_with_TextBlob import label_comments\n",
    "\n",
    "input_filepath = \"./Data/cleaned_comments.csv\"\n",
    "output_filepath = \"./Data/labeled_comments.csv\"\n",
    "\n",
    "df = label_comments(input_filepath)\n",
    "df.to_csv(output_filepath, index=False)\n",
    "print(f\"Labeled comments saved to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching, cleaning and labeling more negative comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1324 negative comments to ./Data/more_negative_comments_new.csv\n"
     ]
    }
   ],
   "source": [
    "from scripts.fetch_comments import fetch_comments\n",
    "from scripts.negative_comment_filter import filter_negative_comments\n",
    "import pandas as pd\n",
    "\n",
    "video_id = \"JfVOs4VSpmA\"\n",
    "all_comments = fetch_comments(video_id, max_results=10000)\n",
    "negative_comments = filter_negative_comments(all_comments)\n",
    "\n",
    "output_filepath = \"./Data/more_negative_comments_new.csv\"\n",
    "negative_comments.to_csv(output_filepath, index=False)\n",
    "print(f\"Saved {len(negative_comments)} negative comments to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1327 comments from ./Data/more_negative_comments_new.csv\n",
      "Filtering non-English comments...\n",
      "Kept 1225 English comments.\n",
      "Removing emoji-only comments...\n",
      "Kept 1225 comments with text.\n",
      "Saved cleaned comments to ./Data/cleaned_negative_comments_new.csv\n",
      "Cleaned negative comments saved to ./Data/cleaned_negative_comments_new.csv\n"
     ]
    }
   ],
   "source": [
    "from cleaning_and_labeling import load_comments, filter_non_english_comments, filter_emoji_only_comments, save_cleaned_comments\n",
    "\n",
    "input_filepath = \"./Data/more_negative_comments_new.csv\" \n",
    "output_filepath = \"./Data/cleaned_negative_comments_new.csv\" \n",
    "\n",
    "df = load_comments(input_filepath)\n",
    "df = filter_non_english_comments(df)\n",
    "df = filter_emoji_only_comments(df)\n",
    "\n",
    "save_cleaned_comments(df, output_filepath)\n",
    "\n",
    "print(f\"Cleaned negative comments saved to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching, cleaning and labeling more neutral comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1021 neutral comments to ./Data/more_neutral_comments.csv\n"
     ]
    }
   ],
   "source": [
    "from scripts.fetch_comments import fetch_comments\n",
    "from scripts.neutral_comment_filter import filter_neutral_comments\n",
    "import pandas as pd\n",
    "\n",
    "video_id = \"JfVOs4VSpmA\"\n",
    "all_comments = fetch_comments(video_id, max_results=2000)\n",
    "neutral_comments = filter_neutral_comments(all_comments)\n",
    "\n",
    "output_filepath = \"./Data/more_neutral_comments.csv\"\n",
    "neutral_comments.to_csv(output_filepath, index=False)\n",
    "print(f\"Saved {len(neutral_comments)} neutral comments to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1021 comments from ./Data/more_neutral_comments.csv\n",
      "Filtering non-English comments...\n",
      "Kept 581 English comments.\n",
      "Removing emoji-only comments...\n",
      "Kept 581 comments with text.\n",
      "Saved cleaned comments to ./Data/cleaned_neutral_comments.csv\n",
      "Cleaned neutral comments saved to ./Data/cleaned_neutral_comments.csv\n"
     ]
    }
   ],
   "source": [
    "from cleaning_and_labeling import load_comments, filter_non_english_comments, filter_emoji_only_comments, save_cleaned_comments\n",
    "\n",
    "input_filepath = \"./Data/more_neutral_comments.csv\" \n",
    "output_filepath = \"./Data/cleaned_neutral_comments.csv\" \n",
    "\n",
    "df = load_comments(input_filepath)\n",
    "df = filter_non_english_comments(df)\n",
    "df = filter_emoji_only_comments(df)\n",
    "\n",
    "save_cleaned_comments(df, output_filepath)\n",
    "\n",
    "print(f\"Cleaned neutral comments saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling cleaned dataset before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset saved to ./Data/shuffled_labeled_normal.csv\n"
     ]
    }
   ],
   "source": [
    "input_filepath = \"./Data/labeled_dataset_normal.csv\" \n",
    "df = pd.read_csv(input_filepath)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "output_filepath = \"./Data/shuffled_labeled_normal.csv\"\n",
    "df.to_csv(output_filepath, index=False)\n",
    "print(f\"Shuffled dataset saved to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and lemmatization complete. Saved to ./Data/tokenized_and_lemmatized_improved.csv\n"
     ]
    }
   ],
   "source": [
    "from scripts.tokenize_and_lemmatize import tokenize, lemmatize\n",
    "\n",
    "input_filepath = \"./Data/shuffled_labeled_improved.csv\"\n",
    "df = pd.read_csv(input_filepath, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# tokenization\n",
    "df[\"Tokens\"] = df[\"Comment\"].apply(tokenize)\n",
    "\n",
    "# lemmatization\n",
    "df[\"Lemmatized_Comment\"] = df[\"Tokens\"].apply(lemmatize)\n",
    "\n",
    "output_filepath = \"./Data/tokenized_and_lemmatized_improved.csv\"\n",
    "df.to_csv(output_filepath, index=False)\n",
    "print(f\"Tokenization and lemmatization complete. Saved to {output_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logestic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.73      0.77       150\n",
      "     neutral       0.73      0.77      0.75       149\n",
      "    positive       0.75      0.78      0.77       150\n",
      "\n",
      "    accuracy                           0.76       449\n",
      "   macro avg       0.76      0.76      0.76       449\n",
      "weighted avg       0.76      0.76      0.76       449\n",
      "\n",
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "input_filepath = \"./Data/tokenized_and_lemmatized_improved.csv\"\n",
    "df = pd.read_csv(input_filepath)\n",
    "\n",
    "#adjustments\n",
    "df = df.dropna()\n",
    "df = df[(df[\"Sentiment\"] != \"negetive\") & (df[\"Sentiment\"].notna())]\n",
    "\n",
    "# Featuring and Labeling\n",
    "X = df[\"Lemmatized_Comment\"]\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "# Split dataset to 30% test and 70% training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1500)  \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Training\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(model, \"./models/logistic_regression_bow.pkl\")\n",
    "joblib.dump(vectorizer, \"./models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to increase the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding best C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f\"Best C: {best_C}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
